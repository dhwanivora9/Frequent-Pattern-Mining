{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Exercise for  FPM Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exploring properties of the dataset accidents_10k.dat. Read more about it here:  http://fimi.uantwerpen.be/data/accidents.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \r\n",
      "2 5 7 8 9 10 12 13 14 15 16 17 18 20 22 23 24 25 27 28 29 32 33 34 35 36 37 38 39 \r\n",
      "7 10 12 13 14 15 16 17 18 20 25 28 29 30 33 40 41 42 43 44 45 46 47 48 49 50 51 52 \r\n",
      "1 5 8 10 12 14 15 16 17 18 19 20 21 22 24 25 26 27 28 29 30 31 41 43 46 48 49 51 52 53 54 55 56 57 58 59 60 61 \r\n",
      "5 8 10 12 14 15 16 17 18 21 22 24 25 26 27 28 29 31 33 36 38 39 41 43 46 56 62 63 64 65 66 67 68 \r\n",
      "7 8 10 12 17 18 21 23 24 26 27 28 29 30 33 34 35 36 38 41 43 47 59 63 66 69 70 71 72 73 74 75 76 77 78 79 \r\n",
      "1 12 14 15 16 17 18 21 22 23 24 25 27 28 29 30 31 35 38 41 43 44 53 56 57 58 59 60 63 66 80 81 82 83 84 \r\n",
      "10 12 14 15 16 17 18 21 22 24 25 26 27 28 29 30 31 33 39 41 43 44 46 49 59 60 62 63 66 82 \r\n",
      "1 8 10 12 14 15 16 17 18 21 22 23 24 25 27 29 30 31 38 41 43 53 56 59 61 63 66 68 85 86 87 88 89 \r\n",
      "1 8 12 13 14 15 16 17 18 22 24 25 28 30 38 41 42 43 46 49 60 63 64 66 80 82 84 90 91 92 93 94 95 \r\n"
     ]
    }
   ],
   "source": [
    "!head accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1a:** </span>. How many items are there in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\r\n"
     ]
    }
   ],
   "source": [
    "!awk -- '{for (i = 1; i <= NF; i++) wc[$i] += 1}; END {print length(wc)}' accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** there are 310 total item sets </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1b:** </span> How many transactions are present in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 accidents_10k.dat\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:**there are 10,000 tarnsactions </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1c:** </span>.  What is the length of the smallest transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\r\n"
     ]
    }
   ],
   "source": [
    "!awk ' {if ( NF < min || NR==1 ) min=NF} END {print min}' accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Length of the smallest tarnsaction is 23 </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1d:** </span>  What is the length of the longest transaction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\r\n"
     ]
    }
   ],
   "source": [
    "!awk '{ if (NF > max) max = NF } END { print max }' accidents_10k.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Length of the longest transaction is 45</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1e:** </span>  What is the size of the search space of frequent itemsets in this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2085924839766513752338888384931203236916703635113918720651407820138886450957656787131798913024"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** The search space for the given database will be 2^310 which is present in the output table</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1f:** </span> \n",
    "Assume that you work for the deparment of transportation that collected this data. What benefit do you see in using itemset mining approaches on this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:**Being in the department of transportation, following dataset can be used to plan future decisions based on the analysing the historic data. We can calculate the common occurences leading to the accidents and try avoiding those combinations in future </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1g:** </span>  What type of itemsets (frequent, maximial or closed) would you be interested in discovering this dataset? State your reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** I would choose closed itemsets mainly because closed itemset is a subset of frequent set and it is also lossless which means it will cover frequency information of all the items and it will give me key items to focus on while preventing accidents</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 1h:** </span>  What minsup threshold would you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** I will use 20-25% of the accidents which have frequent reasons for their occurences therefore 20-25% of 10,000 accidents will be around 2000-2500 as a minsup value </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating frequent, maximal and closed itemsets using $\\color{red}{\\text{Apriori}}$, $\\color{red}{\\text{ECLAT}}$, and $\\color{red}{\\text{FPGrowth}}$ algorihtms from the dataset accidents_10k.dat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2a:** </span> Generate frequent itemsets using Apriori, for minsup = 2000, 3000, and 4000. Which of these minsup thresholds results in a maximum number of frequent itemsets? Which of these minsup thresholds results in a least number of frequent itemsets? Provide a rationale for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ./apriori [options] infile [outfile]\r\n",
      "find frequent item sets with the apriori algorithm\r\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\r\n",
      "-t#      target type                              (default: s)\r\n",
      "         (s: frequent, c: closed, m: maximal item sets,\r\n",
      "          g: generators, r: association rules)\r\n",
      "-m#      minimum number of items per set/rule     (default: 1)\r\n",
      "-n#      maximum number of items per set/rule     (default: no limit)\r\n",
      "-s#      minimum support of an item set/rule      (default: 10%)\r\n",
      "-S#      maximum support of an item set/rule      (default: 100%)\r\n",
      "         (positive: percentage, negative: absolute number)\r\n",
      "-o       use original rule support definition     (body & head)\r\n",
      "-c#      minimum confidence of an assoc. rule     (default: 80%)\r\n",
      "-e#      additional evaluation measure            (default: none)\r\n",
      "-a#      aggregation mode for evaluation measure  (default: none)\r\n",
      "-d#      threshold for add. evaluation measure    (default: 10%)\r\n",
      "-i       invalidate eval. below expected support  (default: evaluate all)\r\n",
      "-p#      (min. size for) pruning with evaluation  (default: no pruning)\r\n",
      "         (< 0: weak forward, > 0 strong forward, = 0: backward pruning)\r\n",
      "-q#      sort items w.r.t. their frequency        (default: 2)\r\n",
      "         (1: ascending, -1: descending, 0: do not sort,\r\n",
      "          2: ascending, -2: descending w.r.t. transaction size sum)\r\n",
      "-u#      filter unused items from transactions    (default: 0.01)\r\n",
      "         (0: do not filter items w.r.t. usage in sets,\r\n",
      "         <0: fraction of removed items for filtering,\r\n",
      "         >0: take execution times ratio into account)\r\n",
      "-x       do not prune with perfect extensions     (default: prune)\r\n",
      "-y       a-posteriori pruning of infrequent item sets\r\n",
      "-T       do not organize transactions as a prefix tree\r\n",
      "-F#:#..  support border for filtering item sets   (default: none)\r\n",
      "         (list of minimum support values, one per item set size,\r\n",
      "         starting at the minimum size, as given with option -m#)\r\n",
      "-R#      read item selection/appearance indicators\r\n",
      "-P#      write a pattern spectrum to a file\r\n",
      "-Z       print item set statistics (number of item sets per size)\r\n",
      "-N       do not pre-format some integer numbers   (default: do)\r\n",
      "-g       write item names in scanable form (quote certain characters)\r\n",
      "-h#      record header  for output                (default: \"\")\r\n",
      "-k#      item separator for output                (default: \" \")\r\n",
      "-I#      implication sign for association rules   (default: \" <- \")\r\n",
      "-v#      output format for set/rule information   (default: \" (%S)\")\r\n",
      "-j#      sort item sets in output by their size   (default: no sorting)\r\n",
      "         (< 0: descending, > 0: ascending order)\r\n",
      "-w       integer transaction weight in last field (default: only items)\r\n",
      "-r#      record/transaction separators            (default: \"\\n\")\r\n",
      "-f#      field /item        separators            (default: \" \\t,\")\r\n",
      "-b#      blank   characters                       (default: \" \\t\\r\")\r\n",
      "-C#      comment characters                       (default: \"#\")\r\n",
      "-!       print additional option information\r\n",
      "infile   file to read transactions from           [required]\r\n",
      "outfile  file to write item sets/assoc. rules to  [optional]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x apriori\n",
    "!./apriori\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [18.88s].\n",
      "writing T_AP_Freq_S2000.txt ... [851034 set(s)] done [0.10s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-2000 accidents_10k.dat T_AP_Freq_S2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [38 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9674/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [24741 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 done [4.37s].\n",
      "writing T_AP_Freq_S3000.txt ... [133799 set(s)] done [0.02s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-3000 accidents_10k.dat T_AP_Freq_S3000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [22267 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [1.34s].\n",
      "writing T_AP_Freq_S4000.txt ... [29501 set(s)] done [0.00s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-4000 accidents_10k.dat T_AP_Freq_S4000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:**From the above code we see that minsup=2000 results in 851034 frequent sets,minsup=3000 results in 133799 frequent sets and minsup=4000 results in 29501 frequent sets. We see here that no. of frequent itemsets is less in higher minsup value as it is obvious that less no. of accidents will have more same attributes resulting in accidents </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2b:** </span>   Using Apriori, compare the execution time for finding frequent itemsets for minsup = 2000, 3000, and 4000. Which of these minsup thresholds takes the least amount of time? Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 done [19.04s].\n",
      "writing <null> ... [851034 set(s)] done [0.01s].\n",
      "19 secs  846307 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-2000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.03s].\n",
      "filtering, sorting and recoding items ... [38 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9674/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [24741 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 done [4.40s].\n",
      "writing <null> ... [133799 set(s)] done [0.00s].\n",
      "4 secs  829009 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-3000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [22267 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [1.27s].\n",
      "writing <null> ... [29501 set(s)] done [0.00s].\n",
      "1 secs  604548 microsecs\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-4000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Least execution time for frequent itemsets is the one with minsup=4000 which is 1.60 secs and it is least mainly because its minsup=4000 and at this minsup no of frequent itemsets are minimum </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2c:** </span> Using Apriori, find the frequent itemsets for minsup = 2000, 3000, and 4000. Determine the number of itemsets for each size (1 to max length of an itemset). What trends do you see that are common for all three minsup thresholds? What trends do you see that are different? Provide a rationale for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     49 1\n",
      "    705 2\n",
      "   5285 3\n",
      "  23745 4\n",
      "  69647 5\n",
      " 139628 6\n",
      " 195730 7\n",
      " 193299 8\n",
      " 133819 9\n",
      "  63937 10\n",
      "  20497 11\n",
      "   4189 12\n",
      "    483 13\n",
      "     21 14\n"
     ]
    }
   ],
   "source": [
    "!awk '{print NF-1}' T_AP_Freq_S2000.txt|sort -n|uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     38 1\n",
      "    468 2\n",
      "   2830 3\n",
      "   9887 4\n",
      "  21779 5\n",
      "  31964 6\n",
      "  32020 7\n",
      "  21862 8\n",
      "   9839 9\n",
      "   2705 10\n",
      "    387 11\n",
      "     20 12\n"
     ]
    }
   ],
   "source": [
    "!awk '{print NF-1}' T_AP_Freq_S3000.txt|sort -n|uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     33 1\r\n",
      "    319 2\r\n",
      "   1492 3\r\n",
      "   4043 4\r\n",
      "   6926 5\r\n",
      "   7751 6\r\n",
      "   5626 7\r\n",
      "   2546 8\r\n",
      "    668 9\r\n",
      "     91 10\r\n",
      "      6 11\r\n"
     ]
    }
   ],
   "source": [
    "!awk '{print NF-1}' T_AP_Freq_S4000.txt|sort -n|uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:**From the above we see that in all three minsup values we see number of frequent sets with lower no. of items are less and then increases in mid and again decreases towards the tail (which has max no. of items). This is primarily because formation of prefix tree in the apriori is similar which is less in the begining increases in between and again tapers at the end</span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2d:** </span>  Using Apriori with minsup=2000, compare the number of frequent, maximal, and closed itemsets. Which is the largest set and which is the smallest set? Provide a rationale for these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "building transaction tree ... [20250 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 done [29.61s].\n",
      "filtering for maximal item sets ... done [0.03s].\n",
      "writing T_AP_Maximal_S2000.txt ... [12330 set(s)] done [0.02s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -tm -s-2000 accidents_10k.dat T_AP_Maximal_S2000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [20250 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 12 13 14 done [29.66s].\n",
      "filtering for closed item sets ... done [0.46s].\n",
      "writing T_AP_Closed_S2000.txt ... [519902 set(s)] done [0.10s].\n"
     ]
    }
   ],
   "source": [
    "!./apriori -tc -s-2000 accidents_10k.dat T_AP_Closed_S2000.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Frequent itemset has the highest number of itemsets followed by closed itemset which has 519902 and maximal has the least number of items sets. This is primarily becasue maximal is a subset of closed and closed is a subset of frequent itemset </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2e:** </span> For a minsup = 2000, compare the execution time for Apriori, ECLAT and FPGrowth. Which of these algorithms took the least amount of time. Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ./eclat [options] infile [outfile]\r\n",
      "find frequent item sets with the eclat algorithm\r\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\r\n",
      "-t#      target type                              (default: s)\r\n",
      "         (s: frequent, c: closed, m: maximal item sets,\r\n",
      "          g: generators, r: association rules)\r\n",
      "-m#      minimum number of items per set/rule     (default: 1)\r\n",
      "-n#      maximum number of items per set/rule     (default: no limit)\r\n",
      "-s#      minimum support of an item set/rule      (default: 10%)\r\n",
      "-S#      maximum support of an item set/rule      (default: 100%)\r\n",
      "         (positive: percentage, negative: absolute number)\r\n",
      "-o       use original rule support definition     (body & head)\r\n",
      "-c#      minimum confidence of an assoc. rule     (default: 80%)\r\n",
      "-e#      additional evaluation measure            (default: none)\r\n",
      "-a#      aggregation mode for evaluation measure  (default: none)\r\n",
      "-d#      threshold for add. evaluation measure    (default: 10%)\r\n",
      "-i       invalidate eval. below expected support  (default: evaluate all)\r\n",
      "-p#      (min. size for) pruning with evaluation  (default: no pruning)\r\n",
      "         (< 0: weak forward, > 0 strong forward, = 0: backward pruning)\r\n",
      "-q#      sort items w.r.t. their frequency        (default: 2)\r\n",
      "         (1: ascending, -1: descending, 0: do not sort,\r\n",
      "          2: ascending, -2: descending w.r.t. transaction size sum)\r\n",
      "-A#      variant of the eclat algorithm to use    (default: 'a')\r\n",
      "-x       do not prune with perfect extensions     (default: prune)\r\n",
      "-l#      number of items for k-items machine      (default: 16)\r\n",
      "         (only for algorithm variants i,r,o,   options -Ai/-Ar/-Ao)\r\n",
      "-j       do not sort items w.r.t. cond. support   (default: sort)\r\n",
      "         (only for algorithm variants i,b,t,d, options -Ai/-Ab/-At/-Ad)\r\n",
      "-y#      check extensions for closed/maximal sets (default: repository)\r\n",
      "         (0: horizontal, > 0: vertical representation)\r\n",
      "         (only with improved tid lists variant, option -Ai)\r\n",
      "-u       do not use head union tail (hut) pruning (default: use hut)\r\n",
      "         (only for maximal item sets, option -tm, not with option -Ab)\r\n",
      "-F#:#..  support border for filtering item sets   (default: none)\r\n",
      "         (list of minimum support values, one per item set size,\r\n",
      "         starting at the minimum size, as given with option -m#)\r\n",
      "-R#      read item selection/appearance indicators\r\n",
      "-P#      write a pattern spectrum to a file\r\n",
      "-Z       print item set statistics (number of item sets per size)\r\n",
      "-N       do not pre-format some integer numbers   (default: do)\r\n",
      "-g       write output in scanable form (quote certain characters)\r\n",
      "-h#      record header  for output                (default: \"\")\r\n",
      "-k#      item separator for output                (default: \" \")\r\n",
      "-I#      implication sign for association rules   (default: \" <- \")\r\n",
      "-v#      output format for item set information   (default: \" (%S)\")\r\n",
      "-w       transaction weight in last field         (default: only items)\r\n",
      "-r#      record/transaction separators            (default: \"\\n\")\r\n",
      "-f#      field /item        separators            (default: \" \\t,\")\r\n",
      "-b#      blank   characters                       (default: \" \\t\\r\")\r\n",
      "-C#      comment characters                       (default: \"#\")\r\n",
      "-T#      file to write transaction identifiers to (default: none)\r\n",
      "-!       print additional option information\r\n",
      "infile   file to read transactions from           [required]\r\n",
      "outfile  file to write item sets/assoc.rules to   [optional]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x eclat\n",
    "!./eclat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "writing T_EC_Freq_S2000.txt ... [851034 set(s)] done [0.37s].\n",
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "writing <null> ... [851034 set(s)] done [0.27s].\n",
      "0 secs  530341 microsecs\n"
     ]
    }
   ],
   "source": [
    "!./eclat -ts -s-2000 accidents_10k.dat T_EC_Freq_S2000.txt\n",
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./eclat -ts -s-2000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ./fpgrowth [options] infile [outfile]\n",
      "find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "-t#      target type                              (default: s)\n",
      "         (s: frequent, c: closed, m: maximal item sets,\n",
      "          g: generators, r: association rules)\n",
      "-m#      minimum number of items per set/rule     (default: 1)\n",
      "-n#      maximum number of items per set/rule     (default: no limit)\n",
      "-s#      minimum support of an item set/rule      (default: 10%)\n",
      "-S#      maximum support of an item set/rule      (default: 100%)\n",
      "         (positive: percentage, negative: absolute number)\n",
      "-o       use original rule support definition     (body & head)\n",
      "-c#      minimum confidence of an assoc. rule     (default: 80%)\n",
      "-e#      additional evaluation measure            (default: none)\n",
      "-a#      aggregation mode for evaluation measure  (default: none)\n",
      "-d#      threshold for add. evaluation measure    (default: 10%)\n",
      "-i       invalidate eval. below expected support  (default: evaluate all)\n",
      "-p#      (min. size for) pruning with evaluation  (default: no pruning)\n",
      "         (< 0: weak forward, > 0 strong forward, = 0: backward pruning)\n",
      "-q#      sort items w.r.t. their frequency        (default: 2)\n",
      "         (1: ascending, -1: descending, 0: do not sort,\n",
      "          2: ascending, -2: descending w.r.t. transaction size sum)\n",
      "-A#      variant of the fpgrowth algorithm to use (default: c)\n",
      "-x       do not prune with perfect extensions     (default: prune)\n",
      "-l#      number of items for k-items machine      (default: 16)\n",
      "         (only for variants s and d, options -As or -Ad)\n",
      "-j       do not sort items w.r.t. cond. support   (default: sort)\n",
      "         (only for algorithm variant c, option -Ac)\n",
      "-u       do not use head union tail (hut) pruning (default: use hut)\n",
      "         (only for maximal item sets, option -tm)\n",
      "-F#:#..  support border for filtering item sets   (default: none)\n",
      "         (list of minimum support values, one per item set size,\n",
      "         starting at the minimum size, as given with option -m#)\n",
      "-R#      read item selection/appearance indicators\n",
      "-P#      write a pattern spectrum to a file\n",
      "-Z       print item set statistics (number of item sets per size)\n",
      "-N       do not pre-format some integer numbers   (default: do)\n",
      "-g       write item names in scanable form (quote certain characters)\n",
      "-h#      record header  for output                (default: \"\")\n",
      "-k#      item separator for output                (default: \" \")\n",
      "-I#      implication sign for association rules   (default: \" <- \")\n",
      "-v#      output format for set/rule information   (default: \" (%S)\")\n",
      "-w       integer transaction weight in last field (default: only items)\n",
      "-r#      record/transaction separators            (default: \"\\n\")\n",
      "-f#      field /item        separators            (default: \" \\t,\")\n",
      "-b#      blank   characters                       (default: \" \\t\\r\")\n",
      "-C#      comment characters                       (default: \"#\")\n",
      "-!       print additional option information\n",
      "infile   file to read transactions from           [required]\n",
      "outfile  file to write item sets/assoc. rules to  [optional]\n",
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "writing T_FP_Freq_S2000.txt ... [851034 set(s)] done [0.17s].\n",
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [49 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9951/10000 transaction(s)] done [0.01s].\n",
      "writing <null> ... [851034 set(s)] done [0.08s].\n",
      "0 secs  356366 microsecs\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x fpgrowth\n",
    "!./fpgrowth\n",
    "!./fpgrowth -ts -s-2000 accidents_10k.dat T_FP_Freq_S2000.txt\n",
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./fpgrowth -ts -s-2000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Fp growth takes the least amount of time amongst the three mainly because in FP growth tree is constructed with all necessary support values without scanning through the entire database. This saves lot of time and reduces the execution time. Instead in apriori the execution time increases exponentionally and in fp growth its linearly  </span> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2f:** </span> For a minsup = 4000, compare the execution time for Apriori, ECLAT and FPGrowth. Which of these algorithms took the least amount of time. Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [22267 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [1.13s].\n",
      "writing T_FP_Freq_S4000.txt ... [29501 set(s)] done [0.00s].\n",
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [22267 node(s)] done [0.01s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 9 10 11 done [1.34s].\n",
      "writing <null> ... [29501 set(s)] done [0.00s].\n",
      "1 secs  613870 microsecs\n",
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.01s].\n",
      "writing T_FP_Freq_S4000.txt ... [29501 set(s)] done [0.02s].\n",
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.01s].\n",
      "writing <null> ... [29501 set(s)] done [0.01s].\n",
      "0 secs  256468 microsecs\n",
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.01s].\n",
      "writing T_EC_Freq_S4000.txt ... [29501 set(s)] done [0.04s].\n",
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [33 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [9381/10000 transaction(s)] done [0.01s].\n",
      "writing <null> ... [29501 set(s)] done [0.03s].\n",
      "0 secs  305021 microsecs\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-4000 accidents_10k.dat T_FP_Freq_S4000.txt\n",
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-4000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");\n",
    "!./fpgrowth -ts -s-4000 accidents_10k.dat T_FP_Freq_S4000.txt\n",
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./fpgrowth -ts -s-4000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");\n",
    "!./eclat -ts -s-4000 accidents_10k.dat T_EC_Freq_S4000.txt\n",
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./eclat -ts -s-4000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** FP growth has taken least time as discussed above that it downt scans through the entire database however the difference between fp growth and eclat's execution time is very less at minsup=4000</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2g:** </span>  For a minsup = 6000, compare the execution time for Apriori, ECLAT and FPGrowth. Which of these algorithms took the least amount of time. Provide a rationale for this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [6478 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 done [0.03s].\n",
      "writing T_FP_Freq_S6000.txt ... [2254 set(s)] done [0.00s].\n",
      "./apriori - find frequent item sets with the apriori algorithm\n",
      "version 6.27 (2017.08.01)        (c) 1996-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.00s].\n",
      "building transaction tree ... [6478 node(s)] done [0.00s].\n",
      "checking subsets of size 1 2 3 4 5 6 7 8 done [0.03s].\n",
      "writing <null> ... [2254 set(s)] done [0.00s].\n",
      "0 secs  277315 microsecs\n",
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.01s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.01s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.00s].\n",
      "writing T_FP_Freq_S6000.txt ... [2254 set(s)] done [0.00s].\n",
      "./fpgrowth - find frequent item sets with the fpgrowth algorithm\n",
      "version 6.17 (2017.05.30)        (c) 2004-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.00s].\n",
      "writing <null> ... [2254 set(s)] done [0.00s].\n",
      "0 secs  248274 microsecs\n",
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.00s].\n",
      "writing T_EC_Freq_S6000.txt ... [2254 set(s)] done [0.01s].\n",
      "./eclat - find frequent item sets with the eclat algorithm\n",
      "version 5.20 (2017.05.30)        (c) 2002-2017   Christian Borgelt\n",
      "reading accidents_10k.dat ... [310 item(s), 10000 transaction(s)] done [0.02s].\n",
      "filtering, sorting and recoding items ... [20 item(s)] done [0.00s].\n",
      "sorting and reducing transactions ... [3216/10000 transaction(s)] done [0.00s].\n",
      "writing <null> ... [2254 set(s)] done [0.01s].\n",
      "0 secs  274588 microsecs\n"
     ]
    }
   ],
   "source": [
    "!./apriori -ts -s-6000 accidents_10k.dat T_FP_Freq_S6000.txt\n",
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./apriori -ts -s-6000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");\n",
    "!./fpgrowth -ts -s-6000 accidents_10k.dat T_FP_Freq_S6000.txt\n",
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./fpgrowth -ts -s-6000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");\n",
    "!./eclat -ts -s-6000 accidents_10k.dat T_EC_Freq_S6000.txt\n",
    "import datetime \n",
    "start = datetime.datetime.now()\n",
    "!./eclat -ts -s-6000 accidents_10k.dat \n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print(elapsed.seconds,\"secs \",elapsed.microseconds,\"microsecs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** FP growth has taken least time as discussed above that it downt scans through the entire database however the difference between fp growth and eclat's execution time is very less at minsup=6000</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 2h:** </span> Fill the following table based on execution times computed in __2e__, __2f__, and __2g__. State your observations on the relative computational efficiency at different support thresholds. Based on your knowledge of these algorithms, provide the reasons behind your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Algorithm                |minsup=2000         |minsup=4000         |minsup=6000         |\n",
    "|----------------------------|--------------------|--------------------|--------------------|    \n",
    "|Apriori                     |      19.846307     |    1.613870        |        0.277315    |\n",
    "|Eclat                       |         0.530341   |    0.305021        |        0.274588    |\n",
    "|FPGrowth                    |            0.305021|      0.256468      |        0.248274    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Fp Growth has always taken the least time as it doesnot scan the databse again and again, however at minsup=6000 the execution time for apriori has reduced considerably. Infact apriori execution time decreases exponentionally with higher minsup i.e at minsup=2000 difference/gap between their execution time was very huge and it keeps on reducing as misup values increases and at minsup=6000 the difference between their execution time is very close. Also according to the formula time complexity formula Apriori = 2^I * D * I and for eclat and fp growth is  2^l * D</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Discovering frequent subsequences and substrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that roads in a Cincinnati are assigned numbers. Participants are enrolled in a transportation study and for every trip they make using their car, the sequence of roads taken are recorded. Trips that involves freeways are excluded. This data is in the file <span style=\"color:blue\">road_seq_data.dat</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3a:** </span>  What 'type' of sequence mining will you perform to determine frequently taken 'paths'? Paths are sequences of roads traveresed consecutively in the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Out of three avaialble sequence mining approaches-level-wise approach, Projection-based aprroach and Vertical approach, i would use vertical approach as it is faster than all.As in this approach full database is not scanned and support values are generated by inverted indices  </span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3b:** </span> How many sequences are there in this sequence database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrefixSpan version 1.00 - Sequential Pattern Miner\r\n",
      "Written by Yasuo Tabei\r\n",
      "\r\n",
      "\r\n",
      "Usage: prefixspan [OPTION]... INFILE\r\n",
      "\r\n",
      "       where [OPTION]...  is a list of zero or more optional arguments\r\n",
      "             INFILE(s)    is the name of the input transaction database\r\n",
      "\r\n",
      "Additional arguments (at most one input file may be specified):\r\n",
      "       -min_sup [minimum support]\r\n",
      "       -max_pat [maximum pattern]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x prefixspan\n",
    "!./prefixspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 road_seq_data.dat\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l road_seq_data.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:**Number of sequences in the database is nothing but the total number of rows present in the database which is 1000 in the present database of sequences </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3c:** </span> What is the size of the alphabet in this sequence database?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1283\r\n"
     ]
    }
   ],
   "source": [
    "!awk -- '{for (i = 1; i <= NF; i++) wc[$i] += 1}; END {print length(wc)}' road_seq_data.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Alphabet according to the defintiton is the total number of unique elements present in our database. So in this case it is 1283 </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3d:** </span> What are the total number of possible subsequences of length 2 in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1646089"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1283*1283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Here total number of elements avaiable is 1283 and at level 2 which is total possible number of subsequnces of length 2 which will be 1283*1283 = 1646089</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3e:** </span> What are the total number of possible substrings of length 2 in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1646089"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1283*1283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** In case of total possible number of substrings we will have to consider 1283*1283 elements</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3f:** </span> Discover frequent __subsequences__ with minsup = 10 and report the number of subsequences discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrefixSpan version 1.00 - Sequential Pattern Miner\r\n",
      "Written by Yasuo Tabei\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!./prefixspan -min_sup 10 road_seq_data.dat| sed -n 'p;n'> subseq_activities_data_minsup_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4589 subseq_activities_data_minsup_10\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l subseq_activities_data_minsup_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Total number of frequent subsequences with min sup 10 is 4589 </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3g:** </span>  Discover frequent __substrings__ with minsup = 10 and report the number of substrings discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ./seqwog [options] infile [outfile]\r\n",
      "find frequent sequences without gaps\r\n",
      "version 3.16 (2016.10.15)        (c) 2010-2016   Christian Borgelt\r\n",
      "-t#      target type                              (default: s)\r\n",
      "         (s: frequent, c: closed, m: maximal sequences, r: rules)\r\n",
      "         (target type 'r' implies -a (all occurrences))\r\n",
      "-m#      minimum number of items per sequence     (default: 1)\r\n",
      "-n#      maximum number of items per sequence     (default: no limit)\r\n",
      "-s#      minimum support of a sequence            (default: 10%)\r\n",
      "         (positive: percentage, negative: absolute number)\r\n",
      "-o       use original rule support definition     (body & head)\r\n",
      "-c#      minimum confidence of a     rule         (default: 80%)\r\n",
      "-a       count all occurrences of a pattern       (default: #sequences)\r\n",
      "-F#:#..  support border for filtering item sets   (default: none)\r\n",
      "         (list of minimum support values, one per item set size,\r\n",
      "         starting at the minimum size, as given with option -m#)\r\n",
      "-P#      write pattern spectrum to a file\r\n",
      "-Z       print item set statistics (number of item sets per size)\r\n",
      "-g       write output in scanable form (quote certain characters)\r\n",
      "-h#      record header  for output                (default: \"\")\r\n",
      "-k#      item separator for output                (default: \" \")\r\n",
      "-I#      implication sign for sequence rules      (default: \" -> \")\r\n",
      "-v#      output format for sequence information   (default: \" (%S)\")\r\n",
      "-w       integer transaction weight in last field (default: only items)\r\n",
      "-r#      record/transaction separators            (default: \"\\n\")\r\n",
      "-f#      field /item        separators            (default: \" \\t,\")\r\n",
      "-b#      blank   characters                       (default: \" \\t\\r\")\r\n",
      "-C#      comment characters                       (default: \"#\")\r\n",
      "-!       print additional option information\r\n",
      "infile   file to read transactions from           [required]\r\n",
      "outfile  file to write frequent sequences to      [optional]\r\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x seqwog\n",
    "!./seqwog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./seqwog - find frequent sequences without gaps\r\n",
      "version 3.16 (2016.10.15)        (c) 2010-2016   Christian Borgelt\r\n",
      "reading road_seq_data.dat ... [1283 item(s), 1000 transaction(s)] done [0.01s].\r\n",
      "recoding items ... [1283 item(s)] done [0.00s].\r\n",
      "reducing and triming transactions ... [844/1000 transaction(s)] done [0.00s].\r\n",
      "writing substring_result_10 ... [613 sequence(s)] done [0.00s].\r\n"
     ]
    }
   ],
   "source": [
    "!./seqwog -ts -s-10 road_seq_data.dat substring_result_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613 substring_result_10\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l substring_result_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95201 13 (10)\r\n",
      "95201 (10)\r\n",
      "77751 (10)\r\n",
      "20483 37613 (10)\r\n",
      "20483 (10)\r\n",
      "3033 (10)\r\n",
      "74826 74848 77336 11 (10)\r\n",
      "74826 74848 77336 (10)\r\n",
      "74826 74848 (10)\r\n",
      "74826 (10)\r\n",
      "77336 11 (10)\r\n",
      "77336 (10)\r\n",
      "40602 (10)\r\n",
      "40191 (10)\r\n",
      "37708 37730 40191 (10)\r\n",
      "37708 37730 (10)\r\n",
      "37708 (10)\r\n",
      "108 130 2618 37611 (10)\r\n",
      "108 130 2618 (10)\r\n",
      "108 130 (10)\r\n",
      "108 (10)\r\n",
      "2618 37611 (10)\r\n",
      "2618 (10)\r\n",
      "37679 (11)\r\n",
      "74797 (11)\r\n",
      "77184 13 16 (11)\r\n",
      "77184 13 (11)\r\n",
      "77184 (11)\r\n",
      "40560 (11)\r\n",
      "79 (11)\r\n",
      "2466 37613 37616 (11)\r\n",
      "2466 37613 (11)\r\n",
      "2466 (11)\r\n",
      "2991 37611 37631 (10)\r\n",
      "2991 37611 (11)\r\n",
      "2991 (11)\r\n",
      "77709 11 31 (10)\r\n",
      "77709 11 (11)\r\n",
      "77709 (11)\r\n",
      "40039 (11)\r\n",
      "74767 (13)\r\n",
      "37641 40560 (11)\r\n",
      "37641 (13)\r\n",
      "74784 (13)\r\n",
      "49 (13)\r\n",
      "39298 (13)\r\n",
      "74759 77709 11 31 (10)\r\n",
      "74759 77709 11 (11)\r\n",
      "74759 77709 (11)\r\n",
      "74759 (13)\r\n",
      "37649 (13)\r\n",
      "37666 (13)\r\n",
      "41 2991 37611 37631 (10)\r\n",
      "41 2991 37611 (11)\r\n",
      "41 2991 (11)\r\n",
      "41 (13)\r\n",
      "66 (13)\r\n",
      "37674 (14)\r\n",
      "76435 13 (12)\r\n",
      "76435 (14)\r\n",
      "1717 37613 (12)\r\n",
      "1717 (14)\r\n",
      "74792 (14)\r\n",
      "74 (14)\r\n",
      "74949 13 (12)\r\n",
      "74949 (14)\r\n",
      "37831 (14)\r\n",
      "231 37613 (12)\r\n",
      "231 (14)\r\n",
      "40579 (15)\r\n",
      "3010 37613 (13)\r\n",
      "3010 (15)\r\n",
      "77728 13 (13)\r\n",
      "77728 (15)\r\n",
      "74891 13 30 (11)\r\n",
      "74891 13 (16)\r\n",
      "74891 (16)\r\n",
      "74888 13 30 64 73 (10)\r\n",
      "74888 13 30 64 (10)\r\n",
      "74888 13 30 (10)\r\n",
      "74888 13 (16)\r\n",
      "74888 (16)\r\n",
      "173 37613 37630 (11)\r\n",
      "173 37613 (16)\r\n",
      "173 (16)\r\n",
      "170 37613 37630 37664 37673 (10)\r\n",
      "170 37613 37630 37664 (10)\r\n",
      "170 37613 37630 (10)\r\n",
      "170 37613 (16)\r\n",
      "170 (16)\r\n",
      "37773 (16)\r\n",
      "37770 (16)\r\n",
      "74867 1 (13)\r\n",
      "74867 (17)\r\n",
      "74744 74867 1 (13)\r\n",
      "74744 74867 (17)\r\n",
      "74744 (17)\r\n",
      "26 149 37601 (13)\r\n",
      "26 149 (17)\r\n",
      "26 (17)\r\n",
      "149 37601 (13)\r\n",
      "149 (17)\r\n",
      "37626 37749 (17)\r\n",
      "37626 (17)\r\n",
      "37749 (17)\r\n",
      "74726 74826 74848 77336 11 (10)\r\n",
      "74726 74826 74848 77336 (10)\r\n",
      "74726 74826 74848 (10)\r\n",
      "74726 74826 (10)\r\n",
      "74726 (20)\r\n",
      "8 108 130 2618 37611 (10)\r\n",
      "8 108 130 2618 (10)\r\n",
      "8 108 130 (10)\r\n",
      "8 108 (10)\r\n",
      "8 (20)\r\n",
      "37608 37708 37730 40191 (10)\r\n",
      "37608 37708 37730 (10)\r\n",
      "37608 37708 (10)\r\n",
      "37608 (20)\r\n",
      "74896 (20)\r\n",
      "178 (20)\r\n",
      "37778 (20)\r\n",
      "74897 13 18 (10)\r\n",
      "74897 13 (16)\r\n",
      "74897 (20)\r\n",
      "179 37613 37618 (10)\r\n",
      "179 37613 (16)\r\n",
      "179 (20)\r\n",
      "37779 (20)\r\n",
      "79279 13 (18)\r\n",
      "79279 (21)\r\n",
      "4561 37613 (18)\r\n",
      "4561 (21)\r\n",
      "41039 (21)\r\n",
      "3473 37613 (17)\r\n",
      "3473 (21)\r\n",
      "42109 (21)\r\n",
      "78191 13 (17)\r\n",
      "78191 (21)\r\n",
      "3320 37613 37618 (13)\r\n",
      "3320 37613 (21)\r\n",
      "3320 (21)\r\n",
      "40886 (21)\r\n",
      "78038 13 18 (13)\r\n",
      "78038 13 (21)\r\n",
      "78038 (21)\r\n",
      "68685 (22)\r\n",
      "31471 37613 37638 (21)\r\n",
      "31471 37613 (21)\r\n",
      "31471 (22)\r\n",
      "106189 13 38 (21)\r\n",
      "106189 13 (21)\r\n",
      "106189 (22)\r\n",
      "74745 78191 13 (17)\r\n",
      "74745 78191 (21)\r\n",
      "74745 (24)\r\n",
      "27 3473 37613 (17)\r\n",
      "27 3473 (21)\r\n",
      "27 (24)\r\n",
      "37627 41039 (21)\r\n",
      "37627 (24)\r\n",
      "37681 37748 (17)\r\n",
      "37681 (26)\r\n",
      "81 148 (17)\r\n",
      "81 (26)\r\n",
      "37748 (27)\r\n",
      "148 (27)\r\n",
      "74866 (27)\r\n",
      "74799 74866 (17)\r\n",
      "74799 (27)\r\n",
      "74791 (30)\r\n",
      "73 (30)\r\n",
      "37673 (30)\r\n",
      "37730 40191 (10)\r\n",
      "37730 (31)\r\n",
      "130 2618 37611 (10)\r\n",
      "130 2618 (10)\r\n",
      "130 (31)\r\n",
      "74848 77336 11 (10)\r\n",
      "74848 77336 (10)\r\n",
      "74848 (32)\r\n",
      "37665 (36)\r\n",
      "37656 40995 (36)\r\n",
      "37656 (36)\r\n",
      "74774 78147 13 18 (12)\r\n",
      "74774 78147 13 42 (10)\r\n",
      "74774 78147 13 (32)\r\n",
      "74774 78147 (36)\r\n",
      "74774 (36)\r\n",
      "74783 (36)\r\n",
      "56 3429 37613 37618 (12)\r\n",
      "56 3429 37613 37642 (10)\r\n",
      "56 3429 37613 (32)\r\n",
      "56 3429 (36)\r\n",
      "56 (36)\r\n",
      "65 (36)\r\n",
      "40995 (37)\r\n",
      "78147 13 18 (12)\r\n",
      "78147 13 42 (10)\r\n",
      "78147 13 (32)\r\n",
      "78147 (37)\r\n",
      "3429 37613 37618 (12)\r\n",
      "3429 37613 37642 (10)\r\n",
      "3429 37613 (32)\r\n",
      "3429 (37)\r\n",
      "150 37613 (17)\r\n",
      "150 (38)\r\n",
      "22 179 37613 37618 (10)\r\n",
      "22 179 37613 (16)\r\n",
      "22 179 (20)\r\n",
      "22 173 37613 37630 (11)\r\n",
      "22 173 37613 (16)\r\n",
      "22 173 (16)\r\n",
      "22 (38)\r\n",
      "37622 37779 (20)\r\n",
      "37622 37773 (16)\r\n",
      "37622 (38)\r\n",
      "37648 37750 (38)\r\n",
      "37648 (38)\r\n",
      "37750 (38)\r\n",
      "74766 74868 13 (17)\r\n",
      "74766 74868 (38)\r\n",
      "74766 (38)\r\n",
      "48 150 37613 (17)\r\n",
      "48 150 (38)\r\n",
      "48 (38)\r\n",
      "74868 13 (17)\r\n",
      "74868 (38)\r\n",
      "74739 74896 (20)\r\n",
      "74739 74888 13 30 64 73 (10)\r\n",
      "74739 74888 13 30 64 (10)\r\n",
      "74739 74888 13 30 (10)\r\n",
      "74739 74888 13 (16)\r\n",
      "74739 74888 (16)\r\n",
      "74739 (38)\r\n",
      "21 178 (20)\r\n",
      "21 170 37613 37630 37664 37673 (10)\r\n",
      "21 170 37613 37630 37664 (10)\r\n",
      "21 170 37613 37630 (10)\r\n",
      "21 170 37613 (16)\r\n",
      "21 170 (16)\r\n",
      "21 (38)\r\n",
      "37621 37778 (20)\r\n",
      "37621 37770 (16)\r\n",
      "37621 (38)\r\n",
      "74740 74897 13 18 (10)\r\n",
      "74740 74897 13 (16)\r\n",
      "74740 74897 (20)\r\n",
      "74740 74891 13 30 (11)\r\n",
      "74740 74891 13 (16)\r\n",
      "74740 74891 (16)\r\n",
      "74740 (38)\r\n",
      "31 81 148 (12)\r\n",
      "31 81 (14)\r\n",
      "31 79 (11)\r\n",
      "31 (43)\r\n",
      "37631 37681 37748 (12)\r\n",
      "37631 37681 (14)\r\n",
      "37631 37679 (11)\r\n",
      "37631 (43)\r\n",
      "74749 74799 74866 (12)\r\n",
      "74749 74799 (15)\r\n",
      "74749 74797 (11)\r\n",
      "74749 (44)\r\n",
      "151 37613 37618 (10)\r\n",
      "151 37613 (30)\r\n",
      "151 (46)\r\n",
      "25 151 37613 37618 (10)\r\n",
      "25 151 37613 (30)\r\n",
      "25 151 (46)\r\n",
      "25 (46)\r\n",
      "74869 13 18 (10)\r\n",
      "74869 13 (30)\r\n",
      "74869 (46)\r\n",
      "74743 74869 13 18 (10)\r\n",
      "74743 74869 13 (30)\r\n",
      "74743 74869 (46)\r\n",
      "74743 (46)\r\n",
      "62 31471 37613 37638 (21)\r\n",
      "62 31471 37613 (21)\r\n",
      "62 31471 (21)\r\n",
      "62 (47)\r\n",
      "37662 68685 (21)\r\n",
      "37662 (47)\r\n",
      "74780 106189 13 38 (21)\r\n",
      "74780 106189 13 (21)\r\n",
      "74780 106189 (21)\r\n",
      "74780 (47)\r\n",
      "74719 5 25 151 37613 37618 (10)\r\n",
      "74719 5 25 151 37613 (30)\r\n",
      "74719 5 25 151 (46)\r\n",
      "74719 5 25 (46)\r\n",
      "74719 5 (46)\r\n",
      "74719 (49)\r\n",
      "37601 (50)\r\n",
      "1 (50)\r\n",
      "37664 37673 (30)\r\n",
      "37664 37674 (14)\r\n",
      "37664 (53)\r\n",
      "74782 74791 (30)\r\n",
      "74782 74792 (14)\r\n",
      "74782 95201 13 (10)\r\n",
      "74782 95201 (10)\r\n",
      "74782 (54)\r\n",
      "64 73 (30)\r\n",
      "64 74 (14)\r\n",
      "64 20483 37613 (10)\r\n",
      "64 20483 (10)\r\n",
      "64 (54)\r\n",
      "11 31 81 148 (12)\r\n",
      "11 31 81 (14)\r\n",
      "11 31 79 (11)\r\n",
      "11 31 (42)\r\n",
      "11 (56)\r\n",
      "37611 37631 37681 37748 (12)\r\n",
      "37611 37631 37681 (14)\r\n",
      "37611 37631 37679 (11)\r\n",
      "37611 37631 (42)\r\n",
      "37611 (56)\r\n",
      "74729 74749 74799 74866 (12)\r\n",
      "74729 74749 74799 (15)\r\n",
      "74729 74749 74797 (11)\r\n",
      "74729 74749 (43)\r\n",
      "74729 (57)\r\n",
      "37616 (59)\r\n",
      "74734 (60)\r\n",
      "16 (60)\r\n",
      "37620 37658 40886 (21)\r\n",
      "37620 37658 42109 (21)\r\n",
      "37620 37658 40579 (15)\r\n",
      "37620 37658 39298 (12)\r\n",
      "37620 37658 (72)\r\n",
      "37620 (79)\r\n",
      "74738 74776 78038 13 18 (13)\r\n",
      "74738 74776 78038 13 (21)\r\n",
      "74738 74776 78038 (21)\r\n",
      "74738 74776 79279 13 (18)\r\n",
      "74738 74776 79279 (21)\r\n",
      "74738 74776 77728 13 (13)\r\n",
      "74738 74776 77728 (15)\r\n",
      "74738 74776 76435 13 (12)\r\n",
      "74738 74776 76435 (13)\r\n",
      "74738 74776 (74)\r\n",
      "74738 (81)\r\n",
      "20 58 3320 37613 37618 (13)\r\n",
      "20 58 3320 37613 (21)\r\n",
      "20 58 3320 (21)\r\n",
      "20 58 4561 37613 (18)\r\n",
      "20 58 4561 (21)\r\n",
      "20 58 3010 37613 (13)\r\n",
      "20 58 3010 (15)\r\n",
      "20 58 1717 37613 (12)\r\n",
      "20 58 1717 (13)\r\n",
      "20 58 (74)\r\n",
      "20 (81)\r\n",
      "38 231 37613 (12)\r\n",
      "38 231 (14)\r\n",
      "38 (89)\r\n",
      "37638 37831 (14)\r\n",
      "37638 (89)\r\n",
      "74756 74949 13 (12)\r\n",
      "74756 74949 (14)\r\n",
      "74756 (89)\r\n",
      "37642 37658 (28)\r\n",
      "37642 37662 68685 (21)\r\n",
      "37642 37662 (47)\r\n",
      "37642 (97)\r\n",
      "74760 74776 (28)\r\n",
      "74760 74782 95201 13 (10)\r\n",
      "74760 74782 95201 (10)\r\n",
      "74760 74782 (10)\r\n",
      "74760 74780 106189 13 38 (21)\r\n",
      "74760 74780 106189 13 (21)\r\n",
      "74760 74780 106189 (21)\r\n",
      "74760 74780 (47)\r\n",
      "74760 (98)\r\n",
      "42 58 (28)\r\n",
      "42 64 20483 37613 (10)\r\n",
      "42 64 20483 (10)\r\n",
      "42 64 (10)\r\n",
      "42 62 31471 37613 37638 (21)\r\n",
      "42 62 31471 37613 (21)\r\n",
      "42 62 31471 (21)\r\n",
      "42 62 (47)\r\n",
      "42 (98)\r\n",
      "37630 37658 (11)\r\n",
      "37630 37664 37673 (30)\r\n",
      "37630 37664 37674 (14)\r\n",
      "37630 37664 (44)\r\n",
      "37630 37665 (36)\r\n",
      "37630 (100)\r\n",
      "74748 74776 (11)\r\n",
      "74748 74782 74791 (30)\r\n",
      "74748 74782 74792 (14)\r\n",
      "74748 74782 (44)\r\n",
      "74748 74783 (36)\r\n",
      "74748 (100)\r\n",
      "30 58 (11)\r\n",
      "30 64 73 (30)\r\n",
      "30 64 74 (14)\r\n",
      "30 64 (44)\r\n",
      "30 65 (36)\r\n",
      "30 (100)\r\n",
      "37658 40886 (21)\r\n",
      "37658 42109 (21)\r\n",
      "37658 40579 (15)\r\n",
      "37658 39298 (12)\r\n",
      "37658 (114)\r\n",
      "58 3320 37613 37618 (13)\r\n",
      "58 3320 37613 (21)\r\n",
      "58 3320 (21)\r\n",
      "58 4561 37613 (18)\r\n",
      "58 4561 (21)\r\n",
      "58 3010 37613 (13)\r\n",
      "58 3010 (15)\r\n",
      "58 1717 37613 (12)\r\n",
      "58 1717 (13)\r\n",
      "58 (116)\r\n",
      "74776 78038 13 18 (13)\r\n",
      "74776 78038 13 (21)\r\n",
      "74776 78038 (21)\r\n",
      "74776 79279 13 (18)\r\n",
      "74776 79279 (21)\r\n",
      "74776 77728 13 (13)\r\n",
      "74776 77728 (15)\r\n",
      "74776 76435 13 (12)\r\n",
      "74776 76435 (13)\r\n",
      "74776 (116)\r\n",
      "37605 37618 (10)\r\n",
      "37605 37621 37778 (20)\r\n",
      "37605 37621 37770 (16)\r\n",
      "37605 37621 (38)\r\n",
      "37605 37648 37750 (38)\r\n",
      "37605 37648 (38)\r\n",
      "37605 37622 37779 (20)\r\n",
      "37605 37622 37773 (16)\r\n",
      "37605 37622 (38)\r\n",
      "37605 37626 37749 (17)\r\n",
      "37605 37626 (17)\r\n",
      "37605 (141)\r\n",
      "18 (151)\r\n",
      "37618 (151)\r\n",
      "74736 (151)\r\n",
      "37607 37620 37658 40886 (21)\r\n",
      "37607 37620 37658 42109 (21)\r\n",
      "37607 37620 37658 40579 (15)\r\n",
      "37607 37620 37658 39298 (12)\r\n",
      "37607 37620 37658 (72)\r\n",
      "37607 37620 (79)\r\n",
      "37607 37656 40995 (36)\r\n",
      "37607 37656 (36)\r\n",
      "37607 37627 41039 (21)\r\n",
      "37607 37627 (24)\r\n",
      "37607 37641 40560 (11)\r\n",
      "37607 37641 (13)\r\n",
      "37607 (168)\r\n",
      "7 20 58 3320 37613 37618 (13)\r\n",
      "7 20 58 3320 37613 (21)\r\n",
      "7 20 58 3320 (21)\r\n",
      "7 20 58 4561 37613 (18)\r\n",
      "7 20 58 4561 (21)\r\n",
      "7 20 58 3010 37613 (13)\r\n",
      "7 20 58 3010 (15)\r\n",
      "7 20 58 1717 37613 (12)\r\n",
      "7 20 58 1717 (13)\r\n",
      "7 20 58 (74)\r\n",
      "7 20 (81)\r\n",
      "7 56 3429 37613 37618 (12)\r\n",
      "7 56 3429 37613 37642 (10)\r\n",
      "7 56 3429 37613 (32)\r\n",
      "7 56 3429 (36)\r\n",
      "7 56 (36)\r\n",
      "7 27 3473 37613 (17)\r\n",
      "7 27 3473 (21)\r\n",
      "7 27 (24)\r\n",
      "7 41 2991 37611 37631 (10)\r\n",
      "7 41 2991 37611 (11)\r\n",
      "7 41 2991 (11)\r\n",
      "7 41 (13)\r\n",
      "7 (170)\r\n",
      "74725 74738 74776 78038 13 18 (13)\r\n",
      "74725 74738 74776 78038 13 (21)\r\n",
      "74725 74738 74776 78038 (21)\r\n",
      "74725 74738 74776 79279 13 (18)\r\n",
      "74725 74738 74776 79279 (21)\r\n",
      "74725 74738 74776 77728 13 (13)\r\n",
      "74725 74738 74776 77728 (15)\r\n",
      "74725 74738 74776 76435 13 (12)\r\n",
      "74725 74738 74776 76435 (13)\r\n",
      "74725 74738 74776 (74)\r\n",
      "74725 74738 (81)\r\n",
      "74725 74774 78147 13 18 (12)\r\n",
      "74725 74774 78147 13 42 (10)\r\n",
      "74725 74774 78147 13 (32)\r\n",
      "74725 74774 78147 (36)\r\n",
      "74725 74774 (36)\r\n",
      "74725 74745 78191 13 (17)\r\n",
      "74725 74745 78191 (21)\r\n",
      "74725 74745 (24)\r\n",
      "74725 74759 77709 11 31 (10)\r\n",
      "74725 74759 77709 11 (11)\r\n",
      "74725 74759 77709 (11)\r\n",
      "74725 74759 (13)\r\n",
      "74725 (170)\r\n",
      "74723 74736 (10)\r\n",
      "74723 74743 74869 13 18 (10)\r\n",
      "74723 74743 74869 13 (30)\r\n",
      "74723 74743 74869 (46)\r\n",
      "74723 74743 (46)\r\n",
      "74723 74740 74897 13 18 (10)\r\n",
      "74723 74740 74897 13 (16)\r\n",
      "74723 74740 74897 (20)\r\n",
      "74723 74740 74891 13 30 (11)\r\n",
      "74723 74740 74891 13 (16)\r\n",
      "74723 74740 74891 (16)\r\n",
      "74723 74740 (38)\r\n",
      "74723 74739 74896 (20)\r\n",
      "74723 74739 74888 13 30 64 73 (10)\r\n",
      "74723 74739 74888 13 30 64 (10)\r\n",
      "74723 74739 74888 13 30 (10)\r\n",
      "74723 74739 74888 13 (16)\r\n",
      "74723 74739 74888 (16)\r\n",
      "74723 74739 (38)\r\n",
      "74723 74766 74868 13 (17)\r\n",
      "74723 74766 74868 (38)\r\n",
      "74723 74766 (38)\r\n",
      "74723 74744 74867 1 (13)\r\n",
      "74723 74744 74867 (17)\r\n",
      "74723 74744 (17)\r\n",
      "74723 (187)\r\n",
      "5 18 (10)\r\n",
      "5 25 151 37613 37618 (10)\r\n",
      "5 25 151 37613 (30)\r\n",
      "5 25 151 (46)\r\n",
      "5 25 (46)\r\n",
      "5 21 178 (20)\r\n",
      "5 21 170 37613 37630 37664 37673 (10)\r\n",
      "5 21 170 37613 37630 37664 (10)\r\n",
      "5 21 170 37613 37630 (10)\r\n",
      "5 21 170 37613 (16)\r\n",
      "5 21 170 (16)\r\n",
      "5 21 (38)\r\n",
      "5 48 150 37613 (17)\r\n",
      "5 48 150 (38)\r\n",
      "5 48 (38)\r\n",
      "5 22 179 37613 37618 (10)\r\n",
      "5 22 179 37613 (16)\r\n",
      "5 22 179 (20)\r\n",
      "5 22 173 37613 37630 (11)\r\n",
      "5 22 173 37613 (16)\r\n",
      "5 22 173 (16)\r\n",
      "5 22 (38)\r\n",
      "5 26 149 37601 (13)\r\n",
      "5 26 149 (17)\r\n",
      "5 26 (17)\r\n",
      "5 (187)\r\n",
      "37613 37618 (138)\r\n",
      "37613 37630 37658 (11)\r\n",
      "37613 37630 37664 37673 (30)\r\n",
      "37613 37630 37664 37674 (14)\r\n",
      "37613 37630 37664 (44)\r\n",
      "37613 37630 37665 (36)\r\n",
      "37613 37630 (100)\r\n",
      "37613 37642 37658 (28)\r\n",
      "37613 37642 37662 68685 (21)\r\n",
      "37613 37642 37662 (47)\r\n",
      "37613 37642 (90)\r\n",
      "37613 37638 37831 (14)\r\n",
      "37613 37638 (89)\r\n",
      "37613 37616 (59)\r\n",
      "37613 (499)\r\n",
      "13 18 (138)\r\n",
      "13 30 58 (11)\r\n",
      "13 30 64 73 (30)\r\n",
      "13 30 64 74 (14)\r\n",
      "13 30 64 (44)\r\n",
      "13 30 65 (36)\r\n",
      "13 30 (100)\r\n",
      "13 42 58 (28)\r\n",
      "13 42 64 20483 37613 (10)\r\n",
      "13 42 64 20483 (10)\r\n",
      "13 42 64 (10)\r\n",
      "13 42 62 31471 37613 37638 (21)\r\n",
      "13 42 62 31471 37613 (21)\r\n",
      "13 42 62 31471 (21)\r\n",
      "13 42 62 (47)\r\n",
      "13 42 (91)\r\n",
      "13 38 231 37613 (12)\r\n",
      "13 38 231 (14)\r\n",
      "13 38 (89)\r\n",
      "13 16 (60)\r\n",
      "13 (501)\r\n",
      "74731 74736 (138)\r\n",
      "74731 74748 74776 (11)\r\n",
      "74731 74748 74782 74791 (30)\r\n",
      "74731 74748 74782 74792 (14)\r\n",
      "74731 74748 74782 (44)\r\n",
      "74731 74748 74783 (36)\r\n",
      "74731 74748 (100)\r\n",
      "74731 74760 74776 (28)\r\n",
      "74731 74760 74782 95201 13 (10)\r\n",
      "74731 74760 74782 95201 (10)\r\n",
      "74731 74760 74782 (10)\r\n",
      "74731 74760 74780 106189 13 38 (21)\r\n",
      "74731 74760 74780 106189 13 (21)\r\n",
      "74731 74760 74780 106189 (21)\r\n",
      "74731 74760 74780 (47)\r\n",
      "74731 74760 (91)\r\n",
      "74731 74756 74949 13 (12)\r\n",
      "74731 74756 74949 (14)\r\n",
      "74731 74756 (89)\r\n",
      "74731 74734 (60)\r\n",
      "74731 (501)\r\n"
     ]
    }
   ],
   "source": [
    "!cat substring_result_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:**Here total number of frequent substring is 613 </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Question 3h:** </span> Explain the difference in the number of frequent subsequences and substrings found in __3f__ and __3g__ above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">**Answer:** Number of subsequenc of min_sup 10 is 4589 and in the case of substring the value is 613. The number of substring is relatively lesser than numebr of subsequence primarily becasue subsequence considers various and all combination of all elements without changing the order of all elements whereas substring considers adjacent elements i.e the order in which they are present. For eg subsequence will have combination of first element with second, first element with third, first with last and so on, however in substring it is just the first element with second,second with third and so on (basically adjacent elements).Another reason for a lesser number of substring values is substring is a part or subset of subsequence which makes it a lesser count </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
